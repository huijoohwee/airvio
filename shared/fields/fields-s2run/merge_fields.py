#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIåŸç”Ÿå­—æ®µåˆå¹¶å·¥å…· - åŸºäºLNSTç²¾ç›Šåˆ›ä¸šç»Ÿç­¹ä¸­æ¢
ä¼˜åŒ–åŸåˆ™ï¼šDRYã€KISSã€YAGNIã€SOLID
ä»·å€¼é“¾è·¯ï¼šç”¨æˆ·ç—›ç‚¹â†’è§£å†³æ–¹æ¡ˆâ†’å•†ä¸šä»·å€¼
å‘å±•è·¯å¾„ï¼šä¸€äººè‰åˆ› â†’ å¤šæ™ºèƒ½ä½“ç¼–æ’ â†’ å¯æŒç»­ç›ˆåˆ©å¢é•¿
"""

import os
import yaml
import json
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
import logging
from pathlib import Path

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class AIFieldMerger:
    """AIåŸç”Ÿå­—æ®µåˆå¹¶å™¨ - ä»·å€¼å¯¼å‘ + ç²¾ç›Šåˆ›ä¸š"""
    
    def __init__(self, fields_dir: str = None, output_file: str = None):
        # ä¿®å¤è·¯å¾„ï¼šæŒ‡å‘fields-s1inç›®å½•
        base_dir = Path(__file__).parent.parent
        self.fields_dir = Path(fields_dir) if fields_dir else base_dir / 'fields-s1in'
        self.output_dir = base_dir / 'fields-s3out'
        self.output_file = Path(output_file) if output_file else self.output_dir / 'fields.yaml'
        self.schema_file = base_dir / 'field_schema.json'
        self.orchestrator_file = base_dir / 'field_orchestrator.yaml'
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.output_dir.mkdir(exist_ok=True)
        
        # ä¼˜å…ˆçº§æ˜ å°„ - éµå¾ªæ ¸å¿ƒåˆ°è¾¹ç¼˜åŸåˆ™
        self.priority_order = {
            'P0': 0,  # æ ¸å¿ƒä»·å€¼é“¾è·¯
            'P1': 1,  # AIåŸç”Ÿæ¶æ„
            'P2': 2,  # è´¨é‡ä¿éšœ
            'P3': 3   # æ‰©å±•åŠŸèƒ½
        }
        
        # AIåä½œçº§åˆ«æ˜ å°„
        self.ai_collaboration_levels = {
            'L0': {'ai_ratio': 90, 'human_ratio': 10, 'use_case': 'æ–‡æ¡£ã€ä»£ç ã€æµ‹è¯•'},
            'L1': {'ai_ratio': 50, 'human_ratio': 50, 'use_case': 'æ¶æ„è®¾è®¡ã€äº§å“å†³ç­–'},
            'L2': {'ai_ratio': 30, 'human_ratio': 70, 'use_case': 'æˆ˜ç•¥è§„åˆ’ã€å•†ä¸šå†³ç­–'}
        }
        
        # ç²¾ç›Šåˆ›ä¸šé˜¶æ®µ
        self.startup_phases = ['discovery', 'validation', 'development', 'launch', 'fundraising']
        
        # ä»·å€¼é“¾è·¯ä½ç½®
        self.value_chain_positions = ['user_pain_point', 'solution_design', 'business_value']
        
        # ç»Ÿè®¡ä¿¡æ¯ - å¢å¼ºç‰ˆ
        self.stats = {
            'total_fields': 0,
            'by_priority': {'P0': 0, 'P1': 0, 'P2': 0, 'P3': 0},
            'by_ai_level': {'L0': 0, 'L1': 0, 'L2': 0},
            'by_startup_phase': {phase: 0 for phase in self.startup_phases},
            'by_value_chain': {pos: 0 for pos in self.value_chain_positions},
            'token_budget_total': 0,
            'mvp_critical_fields': 0,
            'quick_launch_compatible': 0,
            'reusable_templates': 0,
            'business_impact_high': 0,
            'risk_mitigation_covered': 0,
            'validation_errors': 0,
            'files_processed': 0,
            'files_failed': 0
        }
        
        # ä¸šåŠ¡åˆ†æç»“æœ
        self.business_analysis = {
            'high_value_fields': [],
            'mvp_critical_path': [],
            'risk_areas': [],
            'optimization_opportunities': []
        }
    
    def load_schema(self) -> Optional[Dict]:
        """åŠ è½½å­—æ®µSchemaè§„èŒƒ"""
        try:
            if self.schema_file.exists():
                with open(self.schema_file, 'r', encoding='utf-8') as f:
                    schema = json.load(f)
                    logger.info(f"âœ… åŠ è½½SchemaæˆåŠŸ: {self.schema_file}")
                    return schema
        except Exception as e:
            logger.warning(f"âš ï¸ æ— æ³•åŠ è½½Schemaæ–‡ä»¶: {e}")
        return None
    
    def load_orchestrator_config(self) -> Optional[Dict]:
        """åŠ è½½ç¼–æ’å™¨é…ç½®æ–‡ä»¶"""
        try:
            if self.orchestrator_file.exists():
                with open(self.orchestrator_file, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
                    logger.info(f"âœ… åŠ è½½ç¼–æ’å™¨é…ç½®æˆåŠŸ: {self.orchestrator_file}")
                    return config
        except Exception as e:
            logger.warning(f"âš ï¸ æ— æ³•åŠ è½½ç¼–æ’å™¨é…ç½®: {e}")
        return None
    
    def get_file_priority(self, filename: str) -> int:
        """æ ¹æ®æ–‡ä»¶åç¡®å®šä¼˜å…ˆçº§ - å¢å¼ºç‰ˆ"""
        filename_lower = filename.lower()
        
        # P0: æ ¸å¿ƒä»·å€¼é“¾è·¯
        if any(x in filename_lower for x in ['-p0.', 'core', 'lnst', 'mvp']):
            return 0
        
        # P1: AIåŸç”Ÿæ¶æ„
        elif any(x in filename_lower for x in ['-p1.', 'hmnm', 'maos', 'gstr', 'matb']):
            return 1
        
        # P2: è´¨é‡ä¿éšœ
        elif any(x in filename_lower for x in ['-p2.', 'metrics', 'quality']):
            return 2
        
        # P3: æ‰©å±•åŠŸèƒ½
        else:
            return 3
    
    def validate_field(self, field_name: str, field_data: Dict, schema: Dict = None) -> Tuple[bool, List[str]]:
        """éªŒè¯å­—æ®µæ˜¯å¦ç¬¦åˆSchemaè§„èŒƒ - å¢å¼ºç‰ˆ"""
        errors = []
        
        if not isinstance(field_data, dict):
            errors.append("å­—æ®µæ•°æ®å¿…é¡»æ˜¯å­—å…¸ç±»å‹")
            return False, errors
        
        # æ£€æŸ¥å¿…å¡«å­—æ®µ
        required_fields = ['zh-CN', 'en-US', 'description', 'type']
        for req_field in required_fields:
            if req_field not in field_data:
                errors.append(f"ç¼ºå°‘å¿…å¡«å±æ€§: {req_field}")
        
        # æ£€æŸ¥ä¼˜å…ˆçº§
        if 'priority' in field_data:
            if field_data['priority'] not in self.priority_order:
                errors.append(f"ä¼˜å…ˆçº§æ— æ•ˆ: {field_data['priority']}")
        
        # æ£€æŸ¥AIåä½œçº§åˆ«
        if 'ai_collaboration' in field_data:
            if field_data['ai_collaboration'] not in self.ai_collaboration_levels:
                errors.append(f"AIåä½œçº§åˆ«æ— æ•ˆ: {field_data['ai_collaboration']}")
        
        # æ£€æŸ¥ç²¾ç›Šåˆ›ä¸šé˜¶æ®µ
        if 'startup_phase' in field_data:
            if field_data['startup_phase'] not in self.startup_phases:
                errors.append(f"ç²¾ç›Šåˆ›ä¸šé˜¶æ®µæ— æ•ˆ: {field_data['startup_phase']}")
        
        # æ£€æŸ¥ä»·å€¼é“¾è·¯ä½ç½®
        if 'value_chain_position' in field_data:
            if field_data['value_chain_position'] not in self.value_chain_positions:
                errors.append(f"ä»·å€¼é“¾è·¯ä½ç½®æ— æ•ˆ: {field_data['value_chain_position']}")
        
        # æ£€æŸ¥Tokené¢„ç®—æ ¼å¼
        if 'token_budget' in field_data:
            token_budget = field_data['token_budget']
            if isinstance(token_budget, dict):
                if 'estimated_tokens' in token_budget and not isinstance(token_budget['estimated_tokens'], (int, float)):
                    errors.append("Tokené¢„ç®—å¿…é¡»æ˜¯æ•°å­—")
        
        is_valid = len(errors) == 0
        if not is_valid:
            self.stats['validation_errors'] += 1
            logger.warning(f"å­—æ®µ {field_name} éªŒè¯å¤±è´¥: {'; '.join(errors)}")
        
        return is_valid, errors
    
    def analyze_business_value(self, field_name: str, field_data: Dict) -> Dict:
        """åˆ†æå­—æ®µçš„å•†ä¸šä»·å€¼"""
        analysis = {
            'user_value_score': 0,
            'business_value_score': 0,
            'roi_potential': 'unknown',
            'market_validation_status': 'pending'
        }
        
        # åŸºäºä¼˜å…ˆçº§è¯„åˆ†
        priority = field_data.get('priority', 'P3')
        priority_scores = {'P0': 10, 'P1': 7, 'P2': 5, 'P3': 3}
        base_score = priority_scores.get(priority, 3)
        
        # MVPå…³é”®æ€§åŠ åˆ†
        if field_data.get('mvp_relevance', {}).get('is_mvp_critical', False):
            base_score += 5
        
        # å¿«é€Ÿå¯åŠ¨å…¼å®¹æ€§åŠ åˆ†
        if field_data.get('mvp_relevance', {}).get('quick_launch_compatible', False):
            base_score += 3
        
        analysis['user_value_score'] = min(base_score, 10)
        analysis['business_value_score'] = min(base_score, 10)
        
        # åˆ†æå•†ä¸šå½±å“
        if 'business_impact' in field_data:
            business_impact = field_data['business_impact']
            if 'roi_estimate' in business_impact:
                analysis['roi_potential'] = business_impact['roi_estimate']
            if 'market_validation' in business_impact:
                analysis['market_validation_status'] = business_impact['market_validation']
        
        # é«˜ä»·å€¼å­—æ®µè¯†åˆ«
        if analysis['user_value_score'] >= 8 or analysis['business_value_score'] >= 8:
            self.business_analysis['high_value_fields'].append(field_name)
            self.stats['business_impact_high'] += 1
        
        return analysis
    
    def assess_risks(self, field_name: str, field_data: Dict) -> Dict:
        """è¯„ä¼°å­—æ®µç›¸å…³é£é™©"""
        risk_assessment = {
            'technical_risk_level': 'low',
            'market_risk_level': 'low',
            'execution_risk_level': 'low',
            'overall_risk_score': 0
        }
        
        # æ£€æŸ¥é£é™©ç¼“è§£æªæ–½
        if 'risk_mitigation' in field_data:
            risk_mitigation = field_data['risk_mitigation']
            
            # æŠ€æœ¯é£é™©è¯„ä¼°
            tech_risks = risk_mitigation.get('technical_risks', [])
            if len(tech_risks) > 2:
                risk_assessment['technical_risk_level'] = 'high'
            elif len(tech_risks) > 0:
                risk_assessment['technical_risk_level'] = 'medium'
            
            # å¸‚åœºé£é™©è¯„ä¼°
            market_risks = risk_mitigation.get('market_risks', [])
            if len(market_risks) > 2:
                risk_assessment['market_risk_level'] = 'high'
            elif len(market_risks) > 0:
                risk_assessment['market_risk_level'] = 'medium'
            
            # æ‰§è¡Œé£é™©è¯„ä¼°
            exec_risks = risk_mitigation.get('execution_risks', [])
            if len(exec_risks) > 2:
                risk_assessment['execution_risk_level'] = 'high'
            elif len(exec_risks) > 0:
                risk_assessment['execution_risk_level'] = 'medium'
            
            # æœ‰ç¼“è§£ç­–ç•¥çš„å­—æ®µ
            if 'mitigation_strategy' in risk_mitigation:
                self.stats['risk_mitigation_covered'] += 1
        
        # è®¡ç®—æ€»ä½“é£é™©åˆ†æ•°
        risk_levels = {'low': 1, 'medium': 2, 'high': 3}
        total_risk = (risk_levels[risk_assessment['technical_risk_level']] + 
                     risk_levels[risk_assessment['market_risk_level']] + 
                     risk_levels[risk_assessment['execution_risk_level']])
        risk_assessment['overall_risk_score'] = total_risk
        
        # é«˜é£é™©å­—æ®µè¯†åˆ«
        if total_risk >= 7:
            self.business_analysis['risk_areas'].append(field_name)
        
        return risk_assessment
    
    def update_statistics(self, field_name: str, field_data: Dict):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯ - å¢å¼ºç‰ˆ"""
        self.stats['total_fields'] += 1
        
        # æŒ‰ä¼˜å…ˆçº§ç»Ÿè®¡
        priority = field_data.get('priority', 'P3')
        if priority in self.stats['by_priority']:
            self.stats['by_priority'][priority] += 1
        
        # æŒ‰AIåä½œçº§åˆ«ç»Ÿè®¡
        ai_level = field_data.get('ai_collaboration', 'L1')
        if ai_level in self.stats['by_ai_level']:
            self.stats['by_ai_level'][ai_level] += 1
        
        # æŒ‰ç²¾ç›Šåˆ›ä¸šé˜¶æ®µç»Ÿè®¡
        startup_phase = field_data.get('startup_phase')
        if startup_phase and startup_phase in self.stats['by_startup_phase']:
            self.stats['by_startup_phase'][startup_phase] += 1
        
        # æŒ‰ä»·å€¼é“¾è·¯ä½ç½®ç»Ÿè®¡
        value_chain_pos = field_data.get('value_chain_position')
        if value_chain_pos and value_chain_pos in self.stats['by_value_chain']:
            self.stats['by_value_chain'][value_chain_pos] += 1
        
        # Tokené¢„ç®—ç»Ÿè®¡
        if 'token_budget' in field_data and isinstance(field_data['token_budget'], dict):
            estimated_tokens = field_data['token_budget'].get('estimated_tokens', 0)
            if isinstance(estimated_tokens, (int, float)):
                self.stats['token_budget_total'] += estimated_tokens
        
        # MVPå…³é”®å­—æ®µç»Ÿè®¡
        if field_data.get('mvp_relevance', {}).get('is_mvp_critical', False):
            self.stats['mvp_critical_fields'] += 1
            self.business_analysis['mvp_critical_path'].append(field_name)
        
        # å¿«é€Ÿå¯åŠ¨å…¼å®¹æ€§ç»Ÿè®¡
        if field_data.get('mvp_relevance', {}).get('quick_launch_compatible', False):
            self.stats['quick_launch_compatible'] += 1
        
        # å¯å¤ç”¨æ¨¡æ¿ç»Ÿè®¡
        if field_data.get('template_reusability', {}).get('is_reusable', False):
            self.stats['reusable_templates'] += 1
    
    def discover_field_files(self) -> List[Tuple[int, str, Path]]:
        """å‘ç°å¹¶æ’åºå­—æ®µæ–‡ä»¶"""
        file_list = []
        
        if not self.fields_dir.exists():
            logger.error(f"âŒ å­—æ®µç›®å½•ä¸å­˜åœ¨: {self.fields_dir}")
            return file_list
        
        # æ”¯æŒ.yamlå’Œ.ymlæ‰©å±•å
        for pattern in ['*.yaml', '*.yml']:
            for file_path in self.fields_dir.glob(pattern):
                filename = file_path.name
                # è·³è¿‡ç‰¹æ®Šæ–‡ä»¶
                if filename.startswith(('CNTNT-', '_', '.')):
                    continue
                
                priority = self.get_file_priority(filename)
                file_list.append((priority, filename, file_path))
        
        # æŒ‰ä¼˜å…ˆçº§æ’åºï¼šP0 â†’ P1 â†’ P2 â†’ P3
        file_list.sort(key=lambda x: (x[0], x[1]))
        
        logger.info(f"ğŸ“ å‘ç° {len(file_list)} ä¸ªå­—æ®µæ–‡ä»¶")
        return file_list
    
    def merge_fields(self) -> Dict:
        """åˆå¹¶æ‰€æœ‰å­—æ®µæ–‡ä»¶ - å¢å¼ºç‰ˆ"""
        logger.info("ğŸš€ å¼€å§‹AIåŸç”Ÿå­—æ®µåˆå¹¶...")
        
        # åŠ è½½é…ç½®
        schema = self.load_schema()
        orchestrator_config = self.load_orchestrator_config()
        
        merged_data = {}
        file_list = self.discover_field_files()
        
        if not file_list:
            logger.error("âŒ æœªæ‰¾åˆ°ä»»ä½•å­—æ®µæ–‡ä»¶")
            return {}
        
        # åˆå¹¶å­—æ®µ
        for priority, filename, file_path in file_list:
            logger.info(f"ğŸ“„ å¤„ç†æ–‡ä»¶: {filename} (ä¼˜å…ˆçº§: P{priority})")
            
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = yaml.safe_load(f)
                    
                    if not data:
                        logger.warning(f"âš ï¸ æ–‡ä»¶ä¸ºç©º: {filename}")
                        continue
                    
                    self.stats['files_processed'] += 1
                    
                    # å¤„ç†åµŒå¥—ç»“æ„ï¼šæ£€æŸ¥æ˜¯å¦æœ‰ 'fields' é”®
                    fields_data = data
                    if 'fields' in data and isinstance(data['fields'], dict):
                        fields_data = data['fields']
                        logger.info(f"ğŸ“‹ æ£€æµ‹åˆ°åµŒå¥—ç»“æ„ï¼Œå¤„ç† 'fields' é”®ä¸‹çš„å†…å®¹")
                    
                    for field_name, field_data in fields_data.items():
                        # è·³è¿‡å…ƒæ•°æ®å­—æ®µ
                        if field_name.startswith('_') or field_name in ['version', 'meta', 'config', 'fields']:
                            continue
                        
                        # éªŒè¯å­—æ®µ
                        is_valid, errors = self.validate_field(field_name, field_data, schema)
                        
                        if is_valid:
                            # å¤„ç†å­—æ®µå†²çª
                            if field_name in merged_data:
                                existing_priority = merged_data[field_name].get('priority', 'P3')
                                new_priority = field_data.get('priority', 'P3')
                                
                                # é«˜ä¼˜å…ˆçº§è¦†ç›–ä½ä¼˜å…ˆçº§
                                if self.priority_order[new_priority] <= self.priority_order[existing_priority]:
                                    merged_data[field_name] = field_data
                                    logger.info(f"ğŸ”„ å­—æ®µ {field_name} è¢« {new_priority} ä¼˜å…ˆçº§è¦†ç›–")
                            else:
                                merged_data[field_name] = field_data
                                logger.info(f"âœ… æ·»åŠ å­—æ®µ: {field_name}")
                            
                            # æ›´æ–°ç»Ÿè®¡å’Œåˆ†æ
                            self.update_statistics(field_name, field_data)
                            self.analyze_business_value(field_name, field_data)
                            self.assess_risks(field_name, field_data)
                        else:
                            logger.warning(f"âš ï¸ å­—æ®µéªŒè¯å¤±è´¥ï¼Œè·³è¿‡: {field_name} - {'; '.join(errors)}")
                    
            except yaml.YAMLError as e:
                logger.error(f"âŒ YAMLè§£æé”™è¯¯ {filename}: {e}")
                self.stats['files_failed'] += 1
            except Exception as e:
                logger.error(f"âŒ å¤„ç†æ–‡ä»¶é”™è¯¯ {filename}: {e}")
                self.stats['files_failed'] += 1
        
        # ç”Ÿæˆä¼˜åŒ–å»ºè®®
        self.generate_optimization_recommendations()
        
        # æ·»åŠ å…ƒæ•°æ®
        merged_data['_meta'] = {
            'generated_at': datetime.now().isoformat(),
            'generator': 'AIåŸç”Ÿå­—æ®µåˆå¹¶å™¨ v5.1',
            'version': 'v5.1-ai-native-enhanced-fixed',
            'design_principles': ['DRY', 'KISS', 'YAGNI', 'SOLID'],
            'value_chain': 'ç”¨æˆ·ç—›ç‚¹â†’è§£å†³æ–¹æ¡ˆâ†’å•†ä¸šä»·å€¼',
            'development_roadmap': 'ä¸€äººè‰åˆ› â†’ å¤šæ™ºèƒ½ä½“ç¼–æ’ â†’ å¯æŒç»­ç›ˆåˆ©å¢é•¿',
            'statistics': self.stats,
            'business_analysis': self.business_analysis
        }
        
        if orchestrator_config:
            merged_data['_meta']['orchestrator_config'] = orchestrator_config.get('global_config', {})
        
        logger.info(f"âœ… åˆå¹¶å®Œæˆï¼Œå…±å¤„ç† {self.stats['total_fields']} ä¸ªå­—æ®µ")
        return merged_data
    
    def generate_optimization_recommendations(self):
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # Tokenæ•ˆç‡ä¼˜åŒ–å»ºè®®
        if self.stats['token_budget_total'] > 50000:
            recommendations.append("è€ƒè™‘ä¼˜åŒ–Tokenä½¿ç”¨ï¼Œå½“å‰é¢„ç®—è¾ƒé«˜")
        
        # MVPå…³é”®è·¯å¾„ä¼˜åŒ–
        mvp_ratio = (self.stats['mvp_critical_fields'] / max(self.stats['total_fields'], 1)) * 100
        if mvp_ratio < 20:
            recommendations.append("å»ºè®®å¢åŠ MVPå…³é”®å­—æ®µæ¯”ä¾‹ï¼Œå½“å‰æ¯”ä¾‹åä½")
        
        # AIåä½œä¼˜åŒ–
        l0_ratio = (self.stats['by_ai_level'].get('L0', 0) / max(self.stats['total_fields'], 1)) * 100
        if l0_ratio < 50:
            recommendations.append("å»ºè®®å¢åŠ L0çº§AIåä½œå­—æ®µï¼Œæé«˜è‡ªåŠ¨åŒ–ç¨‹åº¦")
        
        # é£é™©ç¼“è§£è¦†ç›–ç‡
        risk_coverage = (self.stats['risk_mitigation_covered'] / max(self.stats['total_fields'], 1)) * 100
        if risk_coverage < 70:
            recommendations.append("å»ºè®®å®Œå–„é£é™©ç¼“è§£ç­–ç•¥è¦†ç›–ç‡")
        
        self.business_analysis['optimization_opportunities'] = recommendations
    
    def save_merged_data(self, merged_data: Dict):
        """ä¿å­˜åˆå¹¶åçš„æ•°æ® - å¢å¼ºç‰ˆ"""
        try:
            with open(self.output_file, 'w', encoding='utf-8') as f:
                # è‡ªå®šä¹‰YAMLè¾“å‡ºæ ¼å¼
                yaml.dump(
                    merged_data, 
                    f, 
                    allow_unicode=True, 
                    sort_keys=False, 
                    default_flow_style=False, 
                    indent=2,
                    width=120
                )
            
            logger.info(f"ğŸ’¾ åˆå¹¶ç»“æœå·²ä¿å­˜åˆ°: {self.output_file}")
            
            # ä¿å­˜åˆ†ææŠ¥å‘Š
            self.save_analysis_report()
            
            # è¾“å‡ºç»Ÿè®¡æŠ¥å‘Š
            self.print_statistics()
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
    
    def save_analysis_report(self):
        """ä¿å­˜åˆ†ææŠ¥å‘Š"""
        report_file = self.output_dir / 'analysis_report.json'
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'statistics': self.stats,
            'business_analysis': self.business_analysis,
            'recommendations': self.business_analysis['optimization_opportunities']
        }
        
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ“Š åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        except Exception as e:
            logger.warning(f"âš ï¸ ä¿å­˜åˆ†ææŠ¥å‘Šå¤±è´¥: {e}")
    
    def print_statistics(self):
        """æ‰“å°ç»Ÿè®¡æŠ¥å‘Š - å¢å¼ºç‰ˆ"""
        print("\n" + "="*80)
        print("ğŸš€ AIåŸç”Ÿå­—æ®µåˆå¹¶ç»Ÿè®¡æŠ¥å‘Š - ä»·å€¼å¯¼å‘ç‰ˆ")
        print("="*80)
        
        # åŸºç¡€ç»Ÿè®¡
        print(f"ğŸ“Š æ€»å­—æ®µæ•°: {self.stats['total_fields']}")
        print(f"ğŸ“ å¤„ç†æ–‡ä»¶: {self.stats['files_processed']} æˆåŠŸ, {self.stats['files_failed']} å¤±è´¥")
        print(f"âš ï¸ éªŒè¯é”™è¯¯: {self.stats['validation_errors']}")
        print(f"ğŸ’° Tokené¢„ç®—æ€»è®¡: {self.stats['token_budget_total']:,}")
        
        # MVPå’Œå¿«é€Ÿå¯åŠ¨
        print(f"\nğŸ¯ MVP & å¿«é€Ÿå¯åŠ¨:")
        print(f"  MVPå…³é”®å­—æ®µ: {self.stats['mvp_critical_fields']}")
        print(f"  å¿«é€Ÿå¯åŠ¨å…¼å®¹: {self.stats['quick_launch_compatible']}")
        print(f"  å¯å¤ç”¨æ¨¡æ¿: {self.stats['reusable_templates']}")
        
        # å•†ä¸šä»·å€¼
        print(f"\nğŸ’¼ å•†ä¸šä»·å€¼åˆ†æ:")
        print(f"  é«˜ä»·å€¼å­—æ®µ: {self.stats['business_impact_high']}")
        print(f"  é£é™©ç¼“è§£è¦†ç›–: {self.stats['risk_mitigation_covered']}")
        
        # ä¼˜å…ˆçº§åˆ†å¸ƒ
        print("\nğŸ“ˆ æŒ‰ä¼˜å…ˆçº§åˆ†å¸ƒ:")
        for priority, count in self.stats['by_priority'].items():
            percentage = (count / max(self.stats['total_fields'], 1) * 100)
            print(f"  {priority}: {count} ({percentage:.1f}%)")
        
        # AIåä½œçº§åˆ«åˆ†å¸ƒ
        print("\nğŸ¤– æŒ‰AIåä½œçº§åˆ«åˆ†å¸ƒ:")
        for level, count in self.stats['by_ai_level'].items():
            percentage = (count / max(self.stats['total_fields'], 1) * 100)
            ai_info = self.ai_collaboration_levels.get(level, {})
            print(f"  {level}: {count} ({percentage:.1f}%) - {ai_info.get('use_case', '')}")
        
        # ç²¾ç›Šåˆ›ä¸šé˜¶æ®µåˆ†å¸ƒ
        print("\nğŸš€ æŒ‰ç²¾ç›Šåˆ›ä¸šé˜¶æ®µåˆ†å¸ƒ:")
        for phase, count in self.stats['by_startup_phase'].items():
            if count > 0:
                percentage = (count / max(self.stats['total_fields'], 1) * 100)
                print(f"  {phase}: {count} ({percentage:.1f}%)")
        
        # ä»·å€¼é“¾è·¯åˆ†å¸ƒ
        print("\nğŸ”— æŒ‰ä»·å€¼é“¾è·¯ä½ç½®åˆ†å¸ƒ:")
        for position, count in self.stats['by_value_chain'].items():
            if count > 0:
                percentage = (count / max(self.stats['total_fields'], 1) * 100)
                print(f"  {position}: {count} ({percentage:.1f}%)")
        
        # ä¼˜åŒ–å»ºè®®
        if self.business_analysis['optimization_opportunities']:
            print("\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
            for i, recommendation in enumerate(self.business_analysis['optimization_opportunities'], 1):
                print(f"  {i}. {recommendation}")
        
        print("\nâœ… åˆå¹¶å®Œæˆï¼éµå¾ªä»·å€¼å¯¼å‘åŸåˆ™ï¼š")
        print("   â€¢ DRY: æ¨¡æ¿åŒ–å¤ç”¨")
        print("   â€¢ KISS: å…ˆèƒ½ç”¨å†å¥½ç”¨")
        print("   â€¢ YAGNI: å½“å‰éœ€æ±‚ä¼˜å…ˆ")
        print("   â€¢ SOLID: é«˜å†…èšä½è€¦åˆ")
        print("   â€¢ ä»·å€¼é“¾è·¯: ç”¨æˆ·ç—›ç‚¹â†’è§£å†³æ–¹æ¡ˆâ†’å•†ä¸šä»·å€¼")
        print("   â€¢ å‘å±•è·¯å¾„: ä¸€äººè‰åˆ›â†’å¤šæ™ºèƒ½ä½“ç¼–æ’â†’å¯æŒç»­ç›ˆåˆ©å¢é•¿")
        print("="*80)
    
    def run(self) -> bool:
        """æ‰§è¡Œåˆå¹¶æµç¨‹"""
        try:
            merged_data = self.merge_fields()
            if merged_data:
                self.save_merged_data(merged_data)
                return True
            return False
        except Exception as e:
            logger.error(f"âŒ åˆå¹¶æµç¨‹æ‰§è¡Œå¤±è´¥: {e}")
            return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨AIåŸç”Ÿå­—æ®µåˆå¹¶å™¨...")
    print("ä»·å€¼é“¾è·¯: ç”¨æˆ·ç—›ç‚¹â†’è§£å†³æ–¹æ¡ˆâ†’å•†ä¸šä»·å€¼")
    print("å‘å±•è·¯å¾„: ä¸€äººè‰åˆ›â†’å¤šæ™ºèƒ½ä½“ç¼–æ’â†’å¯æŒç»­ç›ˆåˆ©å¢é•¿\n")
    
    merger = AIFieldMerger()
    success = merger.run()
    
    if success:
        print("\nğŸ‰ AIåŸç”Ÿå­—æ®µåˆå¹¶æˆåŠŸå®Œæˆï¼")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {merger.output_file}")
        print(f"ğŸ“Š åˆ†ææŠ¥å‘Š: {merger.output_dir / 'analysis_report.json'}")
    else:
        print("âŒ åˆå¹¶è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯")
        exit(1)

if __name__ == '__main__':
    main()